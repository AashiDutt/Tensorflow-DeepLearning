{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Word2Vec using Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import os\n",
    "import math\n",
    "import collections\n",
    "from collections import Counter\n",
    "import errno\n",
    "import random\n",
    "import zipfile\n",
    "import numpy as np\n",
    "from six.moves import urllib, xrange\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Load the Data\n",
    "def fetch_words(url, words_data):\n",
    "    # Make Directory if it does not exists\n",
    "    os.makedirs(words_data, exist_ok=True)\n",
    "    \n",
    "    # Path to Zip File\n",
    "    file_path = os.path.join(words_data, 'words.zip')\n",
    "    \n",
    "    # If zip file is not present, download it\n",
    "    if not os.path.exists(file_path):\n",
    "        print('Downloading Data ...')\n",
    "        urllib.request.urlretrieve(url,file_path)\n",
    "        \n",
    "    # Get data from Zip File\n",
    "    with zipfile.ZipFile(file_path) as f:\n",
    "        print('Loading Data from Zip File ...')\n",
    "        data = f.read(f.namelist()[0])\n",
    "    \n",
    "    # Return a list of all words in data source\n",
    "    return data.decode('ascii').split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Load all Words\n",
    "data_url = 'http://mattmahoney.net/dc/text8.zip'\n",
    "saved_data = './word2vec_data/words/'\n",
    "words = fetch_words(url=data_url, words_data=saved_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Print Sample Words\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Length of Word list\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Print a sentence using Words\n",
    "for w in words[9000:9040]:\n",
    "    print(w, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Word Count\n",
    "def word_count(vocab_size):\n",
    "    # Grab most common words (with count)\n",
    "    vocab = [] + Counter(words).most_common(vocab_size)\n",
    "    # Numpy Array of Most Common Words (without count)\n",
    "    vocab = np.array([w for w, _ in vocab])\n",
    "    # Create Dictionary\n",
    "    dictionary = {word:code for code,word in enumerate(vocab)}\n",
    "    # Create Data\n",
    "    data = np.array([dictionary.get(word,0) for word in words])\n",
    "    return data, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Test the Function\n",
    "data, vocabulary = word_count(vocab_size=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Shape of Data\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vocabulary[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vocabulary.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Word at Index\n",
    "words[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Corresponding Number at same Index\n",
    "data[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to load data in Batches\n",
    "def generate_batches(batch_size, num_skips, skip_window):\n",
    "    global data_idx\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    # Generate a numpy array of values if size batch size\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    # Get Labes corresponding to values in the batch\n",
    "    labels = np.ndarray(shape=(batch_size,1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    \n",
    "    if data_idx + span > len(data):\n",
    "        data_idx = 0\n",
    "    \n",
    "    buffer.extend(data[data_idx:data_idx + span])\n",
    "    data_idx += span\n",
    "    \n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window\n",
    "        targets_to_avoid = [skip_window]\n",
    "        \n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips +j, 0] = buffer[target]\n",
    "        if data_idx == len(data):\n",
    "            buffer[:] = data[:span]\n",
    "            data_idx = span\n",
    "        else:\n",
    "            buffer.append(data[data_idx])\n",
    "            data_idx += 1\n",
    "        \n",
    "        data_idx = (data_idx + len(data) - span) % len(data)\n",
    "        return batch, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "# Batch Size\n",
    "batch_size = 128\n",
    "\n",
    "# Embedding Size: How many dimensions will the embedding vector have\n",
    "embedding_size = 150\n",
    "\n",
    "# Skip Window: How many words to consider to left and right \n",
    "skip_window = 1\n",
    "\n",
    "# Number Skips: How many times to reuse input to generate a variable\n",
    "num_skips = 2\n",
    "\n",
    "# Valid Size: Random set of words to evaluate similarity on\n",
    "valid_size = 16\n",
    "\n",
    "# Valid Window: Pick samples from head of distribution \n",
    "valid_window = 100\n",
    "\n",
    "# Number of words with low numeric ID\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "\n",
    "# Number of -ve examples in sample\n",
    "num_sampled = 64\n",
    "\n",
    "# Number of Training Steps\n",
    "num_steps = 100000\n",
    "\n",
    "# Learning Rate\n",
    "lr = 0.01\n",
    "\n",
    "# Vocabulary Size\n",
    "vocabulary_size = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define Placeholders\n",
    "\n",
    "# Training Inputs [Word ID's]: shape = None [i.e. defined by batch size]\n",
    "train_inputs = tf.placeholder(tf.int32, shape=[None])\n",
    "\n",
    "# Training Labels\n",
    "train_labels = tf.placeholder(tf.int32, shape=[batch_size,1])\n",
    "\n",
    "# Valid Examples\n",
    "valid_dataset = tf.constant(valid_examples, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Variables\n",
    "# Randomly choose values from -1 to 1\n",
    "initial_embedding = tf.random_uniform([vocabulary_size, embedding_size],-1.0,1.0)\n",
    "embeddings = tf.Variable(initial_embedding)\n",
    "embed = tf.nn.embedding_lookup(embeddings, train_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loss Function : NCE Loss\n",
    "nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size], stddev=1.0/np.sqrt(embedding_size)))\n",
    "nce_bias = tf.Variable(tf.zeros([vocabulary_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loss Function\n",
    "loss = tf.reduce_mean(tf.nn.nce_loss(nce_weights,nce_bias, train_labels, embed, num_sampled, vocabulary_size))\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Minibatch Cosine Similarity\n",
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), axis=1, keep_dims=True))\n",
    "# Create Normalized Embeddings\n",
    "normalized_embeddings = embeddings / norm\n",
    "\n",
    "valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "# Get Cosine Similarity\n",
    "similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run the Model\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Initialize Average Loss\n",
    "    average_loss = 0\n",
    "    \n",
    "    for i in range(num_steps):\n",
    "        batch_inptus, batch_labels = generate_batches(batch_size, num_skips, skip_window)\n",
    "        _, loss_val = sess.run([optimizer,loss], feed_dict = {train_inputs: batch_inptus, train_labels: batch_labels})\n",
    "        average_loss += loss_val\n",
    "        \n",
    "        if i%1000 == 0:\n",
    "            if i > 0:\n",
    "                average_loss = average_loss/1000\n",
    "            print('Average Loss at Step {0} is : {1}'.format(i, average_loss))\n",
    "            average_loss = 0\n",
    "        \n",
    "        final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot only 500 words\n",
    "plot_only = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_dim_embed = tsne.fit_transform(final_embeddings[:plot_only,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [vocabulary[i] for i in range(500)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_dim_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Word Embeddings\n",
    "def plot_with labels(low_dim_embs, labels):\n",
    "    assert low_dim_embs.shape[0] >= len(labels), \"More Labels than Embeddings...\"\n",
    "    plt.figure(figsize=(18,18))\n",
    "    for i, label in enumerate(labels):\n",
    "        x,y = low_dim_embs[i,:]\n",
    "        plt.scatter(x,y)\n",
    "        plt.annotate(label, xy=(x,y), xytext=(5,2), textcoords='offset points', ha='right',va='bottom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_with_labels(low_dim_embed, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
